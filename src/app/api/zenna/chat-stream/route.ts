import { NextRequest } from 'next/server';
import { auth } from '@/lib/auth';
import { createMemoryService, MemoryService } from '@/core/services/memory-service';
import { SupabaseIdentityStore } from '@/core/providers/identity/supabase-identity';
import { brainProviderFactory } from '@/core/providers/brain';
import type { Message } from '@/core/interfaces/brain-provider';
import type { UserSettings } from '@/core/interfaces/user-identity';

// Singleton memory service instance
let memoryServiceInstance: MemoryService | null = null;

async function getMemoryService(): Promise<MemoryService> {
  if (!memoryServiceInstance) {
    memoryServiceInstance = createMemoryService();
    await memoryServiceInstance.initialize();
  }
  return memoryServiceInstance;
}

/**
 * Streaming Chat API Endpoint
 *
 * Uses Server-Sent Events (SSE) to stream:
 * 1. Text chunks as they're generated by the LLM
 * 2. Emotion analysis after text is complete
 * 3. Final message for cleanup
 *
 * This enables real-time transcript updates before TTS audio is ready.
 */
export async function POST(request: NextRequest) {
  try {
    // Initialize memory service
    const memoryService = await getMemoryService();
    const identityStore = memoryService.getIdentityStore();

    // Verify authentication using NextAuth
    const session = await auth();

    if (!session?.user?.id) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    const userId = session.user.id;

    const { message } = await request.json();

    if (!message || typeof message !== 'string') {
      return new Response(JSON.stringify({ error: 'Message required' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    // Check for background noise system message
    const isBackgroundNoiseMessage = message.startsWith('[SYSTEM: Background noise');
    let processedMessage = message;

    if (isBackgroundNoiseMessage) {
      // Transform system message into a natural response request
      processedMessage = "I'm detecting some background noise. Please acknowledge this briefly and let me know you'll wait for me to speak clearly. Keep it to one short sentence.";
    }

    // Get user and master config
    const [user, masterConfig] = await Promise.all([
      identityStore.getUser(userId),
      identityStore.getMasterConfig(),
    ]);

    if (!user) {
      return new Response(JSON.stringify({ error: 'User not found' }), {
        status: 404,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    // Get conversation history (permanent, never deleted)
    const storedHistory = await memoryService.getConversationHistory(userId);

    // Build message history for LLM
    const systemPrompt = buildSystemPrompt(masterConfig, user.settings);
    const history: Message[] = [
      { role: 'system', content: systemPrompt },
    ];

    // Inject relevant memories from semantic search (if Pinecone is configured)
    const memoryContext = await memoryService.buildMemoryContext(userId, message);
    if (memoryContext) {
      history.push({
        role: 'system',
        content: memoryContext,
      });
    }

    // Add recent conversation history (last 50 turns for context window management)
    // NOTE: All history is preserved permanently, we just limit what we send to LLM
    const recentHistory = storedHistory.slice(-50);
    for (const turn of recentHistory) {
      if (turn.role === 'user' || turn.role === 'assistant') {
        history.push({
          role: turn.role,
          content: turn.content,
          timestamp: new Date(turn.created_at),
        });
      }
    }

    // Add current user message (use processed message for background noise handling)
    history.push({
      role: 'user',
      content: processedMessage,
      timestamp: new Date(),
    });

    // Save user message to permanent storage (Supabase + Pinecone if configured)
    // NOTE: Memories are PERMANENT - we never delete them
    // Don't save system messages like background noise detection
    if (!isBackgroundNoiseMessage) {
      await memoryService.addConversationTurn(userId, 'user', message);
    }

    // Get brain provider
    const brainProviderId = user.settings.preferredBrainProvider || masterConfig.defaultBrain.providerId || 'gemini-2.5-flash';
    const brainApiKey = user.settings.brainApiKey || masterConfig.defaultBrain.apiKey || process.env.GOOGLE_AI_API_KEY;

    if (!brainApiKey) {
      return new Response(JSON.stringify({ error: 'LLM not configured' }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    const brainProvider = brainProviderFactory.create(brainProviderId, {
      apiKey: brainApiKey,
    });

    // Create SSE stream
    const encoder = new TextEncoder();
    let fullResponse = '';

    const stream = new ReadableStream({
      async start(controller) {
        try {
          // Check if provider supports streaming
          if ('generateResponseStream' in brainProvider && typeof brainProvider.generateResponseStream === 'function') {
            // Use streaming response
            const responseStream = brainProvider.generateResponseStream(history);

            for await (const chunk of responseStream) {
              fullResponse += chunk;

              // Send text chunk
              const event = `data: ${JSON.stringify({ type: 'text', content: chunk })}\n\n`;
              controller.enqueue(encoder.encode(event));
            }
          } else {
            // Fall back to non-streaming
            const response = await brainProvider.generateResponse(history);
            fullResponse = response.content;

            // Send complete response
            const event = `data: ${JSON.stringify({ type: 'text', content: fullResponse })}\n\n`;
            controller.enqueue(encoder.encode(event));
          }

          // Process any action blocks
          const actionResult = await processActionBlocks(fullResponse, userId, user.settings);
          let finalResponse = fullResponse;
          if (actionResult) {
            finalResponse = actionResult.cleanedResponse;
          }

          // Save assistant response to permanent storage (Supabase + Pinecone if configured)
          // NOTE: Memories are PERMANENT - we never delete them unless explicitly requested
          await memoryService.addConversationTurn(userId, 'assistant', finalResponse);

          // Analyze emotion
          const emotion = analyzeEmotion(finalResponse);

          // Send completion event with emotion
          const completeEvent = `data: ${JSON.stringify({
            type: 'complete',
            fullResponse: finalResponse,
            emotion,
          })}\n\n`;
          controller.enqueue(encoder.encode(completeEvent));

          controller.close();
        } catch (error) {
          console.error('Streaming error:', error);
          const errorEvent = `data: ${JSON.stringify({
            type: 'error',
            error: 'Failed to generate response',
          })}\n\n`;
          controller.enqueue(encoder.encode(errorEvent));
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });
  } catch (error) {
    console.error('Chat stream error:', error);
    return new Response(JSON.stringify({ error: 'Failed to process message' }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
}

// Process action blocks (same as in chat/route.ts)
async function processActionBlocks(
  responseContent: string,
  userId: string,
  userSettings: UserSettings
): Promise<{ cleanedResponse: string; actionConfirmation?: string } | null> {
  const jsonBlockRegex = /```json\s*(\{[\s\S]*?\})\s*```/g;
  const matches = [...responseContent.matchAll(jsonBlockRegex)];

  if (matches.length === 0) {
    return null;
  }

  let actionConfirmation: string | undefined;
  const { RoutineStore } = await import('@/core/providers/routines/routine-store');

  for (const match of matches) {
    try {
      const actionData = JSON.parse(match[1]);

      if (actionData.action === 'create_schedule') {
        const routineStore = new RoutineStore({
          supabaseUrl: process.env.NEXT_PUBLIC_SUPABASE_URL!,
          supabaseKey: process.env.SUPABASE_SERVICE_ROLE_KEY!,
        });

        const scheduleType = actionData.schedule_type || 'daily';
        await routineStore.createRoutine({
          userId,
          integrationId: actionData.integration,
          actionId: actionData.actionId,
          name: `${actionData.actionId === 'turn-on' ? 'Turn on' : actionData.actionId === 'turn-off' ? 'Turn off' : 'Activate'} ${actionData.parameters?.target || 'lights'} at ${actionData.time}`,
          description: `Scheduled ${scheduleType} routine`,
          schedule: {
            type: scheduleType,
            time: actionData.time,
            daysOfWeek: actionData.daysOfWeek,
          },
          parameters: actionData.parameters || {},
          enabled: true,
        });

        actionConfirmation = `I've set up a ${scheduleType} schedule to ${actionData.actionId === 'turn-on' ? 'turn on' : actionData.actionId === 'turn-off' ? 'turn off' : 'control'} your ${actionData.parameters?.target || 'lights'} at ${actionData.time}.`;
      }
    } catch (error) {
      console.error('Failed to process action block:', error);
    }
  }

  const cleanedResponse = responseContent.replace(jsonBlockRegex, '').trim();

  return {
    cleanedResponse: actionConfirmation || cleanedResponse,
    actionConfirmation,
  };
}

// Emotion analysis (same as in chat/route.ts)
type EmotionType =
  | 'joy' | 'trust' | 'fear' | 'surprise' | 'sadness' | 'anticipation' | 'anger' | 'disgust'
  | 'neutral' | 'curious' | 'helpful' | 'empathetic' | 'thoughtful' | 'encouraging' | 'calming' | 'focused';

function analyzeEmotion(text: string): EmotionType {
  const lowerText = text.toLowerCase();

  const emotionPatterns: { emotion: EmotionType; patterns: RegExp[]; weight: number }[] = [
    {
      emotion: 'joy',
      patterns: [/\b(happy|glad|delighted|excited|wonderful|fantastic|amazing|great news|congratulations|celebrate|joy|yay|awesome|excellent)\b/i],
      weight: 1.2
    },
    {
      emotion: 'helpful',
      patterns: [/\b(here's how|let me help|i can assist|steps to|guide you|help you|show you how|explain)\b/i, /\d+\.\s+/],
      weight: 1.3
    },
    {
      emotion: 'curious',
      patterns: [/\b(interesting|fascinating|intriguing|wonder|curious|explore|discover)\b/i, /\?$/],
      weight: 1.1
    },
    {
      emotion: 'empathetic',
      patterns: [/\b(understand|feel|hear you|acknowledge|appreciate|that must be|sounds like)\b/i],
      weight: 1.2
    },
    {
      emotion: 'thoughtful',
      patterns: [/\b(consider|reflect|think about|perspective|nuanced|complex|depends|however)\b/i],
      weight: 1.0
    },
    {
      emotion: 'encouraging',
      patterns: [/\b(you can do|believe in|great job|well done|keep going|proud|progress)\b/i],
      weight: 1.2
    },
    {
      emotion: 'calming',
      patterns: [/\b(relax|calm|peace|gentle|easy|no rush|take your time|no worries)\b/i],
      weight: 1.1
    },
    {
      emotion: 'focused',
      patterns: [/\b(specifically|precisely|exactly|detail|focus|important|key point|critical)\b/i],
      weight: 1.0
    },
  ];

  const scores: { emotion: EmotionType; score: number }[] = emotionPatterns.map(({ emotion, patterns, weight }) => {
    let score = 0;
    for (const pattern of patterns) {
      const matches = lowerText.match(pattern);
      if (matches) {
        score += matches.length * weight;
      }
    }
    return { emotion, score };
  });

  scores.sort((a, b) => b.score - a.score);

  if (scores[0].score < 0.5) {
    if (lowerText.length > 100 || /\b(here|this|that|you|your)\b/i.test(lowerText)) {
      return 'helpful';
    }
    return 'neutral';
  }

  return scores[0].emotion;
}

/**
 * Build the complete system prompt with proper hierarchy:
 * 1. MASTER PROMPT (Super Admin defined) - Foundational, cannot be overridden
 * 2. IMMUTABLE RULES - Absolute rules that apply to all users
 * 3. USER PROMPT (Personal) - Additive preferences, cannot conflict with master
 *
 * IMPORTANT: If user prompt conflicts with master guidelines, master wins.
 */
function buildSystemPrompt(
  masterConfig: Awaited<ReturnType<SupabaseIdentityStore['getMasterConfig']>>,
  userSettings: UserSettings
): string {
  // Start with the Master Prompt (foundational)
  let prompt = `=== MASTER GUIDELINES (IMMUTABLE - ALWAYS FOLLOW) ===\n\n${masterConfig.systemPrompt}`;

  // Add immutable rules
  if (masterConfig.immutableRules.length > 0) {
    prompt += `\n\n=== IMMUTABLE RULES (NEVER VIOLATE) ===\n${masterConfig.immutableRules
      .map((r, i) => `${i + 1}. ${r}`)
      .join('\n')}`;
  }

  // Add guardrails
  if (masterConfig.guardrails.blockedTopics?.length) {
    prompt += `\n\nBlocked topics (never discuss): ${masterConfig.guardrails.blockedTopics.join(', ')}`;
  }

  // Add integrations context
  const connectedIntegrations: string[] = [];

  if (userSettings.integrations?.hue?.accessToken) {
    connectedIntegrations.push('hue');
    prompt += `\n\n## Philips Hue Integration (CONNECTED)
You can control the user's Philips Hue lights.`;
  }

  if (userSettings.externalContext?.notion?.token) {
    connectedIntegrations.push('notion');
    prompt += `\n\n## Notion Integration (CONNECTED)
The user has connected their Notion workspace.`;
  }

  if (connectedIntegrations.length > 0) {
    prompt += `\n\nConnected integrations: ${connectedIntegrations.join(', ')}`;
  }

  // Add User Personal Prompt (additive, cannot override master)
  if (userSettings.personalPrompt) {
    prompt += `\n\n=== USER PERSONAL PREFERENCES (ADDITIVE) ===
The following are the user's personal preferences. Follow these UNLESS they conflict with the Master Guidelines above.
If there is any conflict, the Master Guidelines always win.

${userSettings.personalPrompt}`;
  }

  return prompt;
}
