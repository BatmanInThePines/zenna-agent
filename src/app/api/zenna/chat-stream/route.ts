import { NextRequest } from 'next/server';
import { auth } from '@/lib/auth';
import { createMemoryService, MemoryService } from '@/core/services/memory-service';
import { SupabaseIdentityStore } from '@/core/providers/identity/supabase-identity';
import { brainProviderFactory } from '@/core/providers/brain';
import type { Message } from '@/core/interfaces/brain-provider';
import type { UserSettings } from '@/core/interfaces/user-identity';

// Singleton memory service instance
let memoryServiceInstance: MemoryService | null = null;

async function getMemoryService(): Promise<MemoryService> {
  if (!memoryServiceInstance) {
    memoryServiceInstance = createMemoryService();
    await memoryServiceInstance.initialize();
  }
  return memoryServiceInstance;
}

/**
 * Streaming Chat API Endpoint
 *
 * Uses Server-Sent Events (SSE) to stream:
 * 1. Text chunks as they're generated by the LLM
 * 2. Emotion analysis after text is complete
 * 3. Final message for cleanup
 *
 * This enables real-time transcript updates before TTS audio is ready.
 */
export async function POST(request: NextRequest) {
  try {
    // Initialize memory service
    const memoryService = await getMemoryService();
    const identityStore = memoryService.getIdentityStore();

    // Verify authentication using NextAuth
    const session = await auth();

    if (!session?.user?.id) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    const userId = session.user.id;

    const { message } = await request.json();

    if (!message || typeof message !== 'string') {
      return new Response(JSON.stringify({ error: 'Message required' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    // Check for background noise system message
    const isBackgroundNoiseMessage = message.startsWith('[SYSTEM: Background noise');
    let processedMessage = message;

    if (isBackgroundNoiseMessage) {
      // Transform system message into a natural response request
      processedMessage = "I'm detecting some background noise. Please acknowledge this briefly and let me know you'll wait for me to speak clearly. Keep it to one short sentence.";
    }

    // Get user and master config
    const [user, masterConfig] = await Promise.all([
      identityStore.getUser(userId),
      identityStore.getMasterConfig(),
    ]);

    if (!user) {
      return new Response(JSON.stringify({ error: 'User not found' }), {
        status: 404,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    // Get conversation history (permanent, never deleted)
    const storedHistory = await memoryService.getConversationHistory(userId);

    // Build message history for LLM
    const systemPrompt = buildSystemPrompt(masterConfig, user.settings);
    const history: Message[] = [
      { role: 'system', content: systemPrompt },
    ];

    // Inject relevant memories from semantic search (ElevenLabs best practice: retrieveMemories at start of turn)
    const memoryContext = await memoryService.buildMemoryContext(userId, message);
    if (memoryContext) {
      // ElevenLabs pattern: Inject memory context as a separate system message
      // This ensures the LLM has access to relevant past information
      history.push({
        role: 'system',
        content: `# Retrieved Memories (USE THIS INFORMATION)

The following memories have been retrieved based on the current conversation context. You MUST use this information when responding. This step is important.

${memoryContext}

IMPORTANT: If the user asks about something mentioned in the memories above, USE that information. Never say "I don't have information about that" if the information is provided above.`,
      });
    } else {
      // ElevenLabs fallback logic: Handle empty memory gracefully
      history.push({
        role: 'system',
        content: `# Memory Status

No previous memories found related to this topic. If the user shares important information (family members, preferences, significant events), make note of it for future conversations.`,
      });
    }

    // Add recent conversation history (last 50 turns for context window management)
    // NOTE: All history is preserved permanently, we just limit what we send to LLM
    const recentHistory = storedHistory.slice(-50);
    for (const turn of recentHistory) {
      if (turn.role === 'user' || turn.role === 'assistant') {
        history.push({
          role: turn.role,
          content: turn.content,
          timestamp: new Date(turn.created_at),
        });
      }
    }

    // Add current user message (use processed message for background noise handling)
    history.push({
      role: 'user',
      content: processedMessage,
      timestamp: new Date(),
    });

    // Save user message to permanent storage (Supabase + Pinecone if configured)
    // NOTE: Memories are PERMANENT - we never delete them
    // Don't save system messages like background noise detection
    if (!isBackgroundNoiseMessage) {
      await memoryService.addConversationTurn(userId, 'user', message);
    }

    // Get brain provider
    const brainProviderId = user.settings.preferredBrainProvider || masterConfig.defaultBrain.providerId || 'gemini-2.5-flash';
    const brainApiKey = user.settings.brainApiKey || masterConfig.defaultBrain.apiKey || process.env.GOOGLE_AI_API_KEY;

    if (!brainApiKey) {
      return new Response(JSON.stringify({ error: 'LLM not configured' }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    const brainProvider = brainProviderFactory.create(brainProviderId, {
      apiKey: brainApiKey,
    });

    // Create SSE stream
    const encoder = new TextEncoder();
    let fullResponse = '';

    const stream = new ReadableStream({
      async start(controller) {
        try {
          // Check if provider supports streaming
          if ('generateResponseStream' in brainProvider && typeof brainProvider.generateResponseStream === 'function') {
            // Use streaming response
            const responseStream = brainProvider.generateResponseStream(history);

            for await (const chunk of responseStream) {
              fullResponse += chunk;

              // Send text chunk
              const event = `data: ${JSON.stringify({ type: 'text', content: chunk })}\n\n`;
              controller.enqueue(encoder.encode(event));
            }
          } else {
            // Fall back to non-streaming
            const response = await brainProvider.generateResponse(history);
            fullResponse = response.content;

            // Send complete response
            const event = `data: ${JSON.stringify({ type: 'text', content: fullResponse })}\n\n`;
            controller.enqueue(encoder.encode(event));
          }

          // Process any action blocks
          const actionResult = await processActionBlocks(fullResponse, userId, user.settings);
          let finalResponse = fullResponse;
          if (actionResult) {
            finalResponse = actionResult.cleanedResponse;
          }

          // Save assistant response to permanent storage (Supabase + Pinecone if configured)
          // NOTE: Memories are PERMANENT - we never delete them unless explicitly requested
          await memoryService.addConversationTurn(userId, 'assistant', finalResponse);

          // Analyze emotion
          const emotion = analyzeEmotion(finalResponse);

          // Send completion event with emotion
          const completeEvent = `data: ${JSON.stringify({
            type: 'complete',
            fullResponse: finalResponse,
            emotion,
          })}\n\n`;
          controller.enqueue(encoder.encode(completeEvent));

          controller.close();
        } catch (error) {
          console.error('Streaming error:', error);
          const errorEvent = `data: ${JSON.stringify({
            type: 'error',
            error: 'Failed to generate response',
          })}\n\n`;
          controller.enqueue(encoder.encode(errorEvent));
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });
  } catch (error) {
    console.error('Chat stream error:', error);
    return new Response(JSON.stringify({ error: 'Failed to process message' }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
}

// Process action blocks (same as in chat/route.ts)
async function processActionBlocks(
  responseContent: string,
  userId: string,
  userSettings: UserSettings
): Promise<{ cleanedResponse: string; actionConfirmation?: string } | null> {
  const jsonBlockRegex = /```json\s*(\{[\s\S]*?\})\s*```/g;
  const matches = [...responseContent.matchAll(jsonBlockRegex)];

  if (matches.length === 0) {
    return null;
  }

  let actionConfirmation: string | undefined;
  const { RoutineStore } = await import('@/core/providers/routines/routine-store');

  for (const match of matches) {
    try {
      const actionData = JSON.parse(match[1]);

      if (actionData.action === 'create_schedule') {
        const routineStore = new RoutineStore({
          supabaseUrl: process.env.NEXT_PUBLIC_SUPABASE_URL!,
          supabaseKey: process.env.SUPABASE_SERVICE_ROLE_KEY!,
        });

        const scheduleType = actionData.schedule_type || 'daily';
        await routineStore.createRoutine({
          userId,
          integrationId: actionData.integration,
          actionId: actionData.actionId,
          name: `${actionData.actionId === 'turn-on' ? 'Turn on' : actionData.actionId === 'turn-off' ? 'Turn off' : 'Activate'} ${actionData.parameters?.target || 'lights'} at ${actionData.time}`,
          description: `Scheduled ${scheduleType} routine`,
          schedule: {
            type: scheduleType,
            time: actionData.time,
            daysOfWeek: actionData.daysOfWeek,
          },
          parameters: actionData.parameters || {},
          enabled: true,
        });

        actionConfirmation = `I've set up a ${scheduleType} schedule to ${actionData.actionId === 'turn-on' ? 'turn on' : actionData.actionId === 'turn-off' ? 'turn off' : 'control'} your ${actionData.parameters?.target || 'lights'} at ${actionData.time}.`;
      }
    } catch (error) {
      console.error('Failed to process action block:', error);
    }
  }

  const cleanedResponse = responseContent.replace(jsonBlockRegex, '').trim();

  return {
    cleanedResponse: actionConfirmation || cleanedResponse,
    actionConfirmation,
  };
}

// Emotion analysis (same as in chat/route.ts)
type EmotionType =
  | 'joy' | 'trust' | 'fear' | 'surprise' | 'sadness' | 'anticipation' | 'anger' | 'disgust'
  | 'neutral' | 'curious' | 'helpful' | 'empathetic' | 'thoughtful' | 'encouraging' | 'calming' | 'focused';

function analyzeEmotion(text: string): EmotionType {
  const lowerText = text.toLowerCase();

  const emotionPatterns: { emotion: EmotionType; patterns: RegExp[]; weight: number }[] = [
    {
      emotion: 'joy',
      patterns: [/\b(happy|glad|delighted|excited|wonderful|fantastic|amazing|great news|congratulations|celebrate|joy|yay|awesome|excellent)\b/i],
      weight: 1.2
    },
    {
      emotion: 'helpful',
      patterns: [/\b(here's how|let me help|i can assist|steps to|guide you|help you|show you how|explain)\b/i, /\d+\.\s+/],
      weight: 1.3
    },
    {
      emotion: 'curious',
      patterns: [/\b(interesting|fascinating|intriguing|wonder|curious|explore|discover)\b/i, /\?$/],
      weight: 1.1
    },
    {
      emotion: 'empathetic',
      patterns: [/\b(understand|feel|hear you|acknowledge|appreciate|that must be|sounds like)\b/i],
      weight: 1.2
    },
    {
      emotion: 'thoughtful',
      patterns: [/\b(consider|reflect|think about|perspective|nuanced|complex|depends|however)\b/i],
      weight: 1.0
    },
    {
      emotion: 'encouraging',
      patterns: [/\b(you can do|believe in|great job|well done|keep going|proud|progress)\b/i],
      weight: 1.2
    },
    {
      emotion: 'calming',
      patterns: [/\b(relax|calm|peace|gentle|easy|no rush|take your time|no worries)\b/i],
      weight: 1.1
    },
    {
      emotion: 'focused',
      patterns: [/\b(specifically|precisely|exactly|detail|focus|important|key point|critical)\b/i],
      weight: 1.0
    },
  ];

  const scores: { emotion: EmotionType; score: number }[] = emotionPatterns.map(({ emotion, patterns, weight }) => {
    let score = 0;
    for (const pattern of patterns) {
      const matches = lowerText.match(pattern);
      if (matches) {
        score += matches.length * weight;
      }
    }
    return { emotion, score };
  });

  scores.sort((a, b) => b.score - a.score);

  if (scores[0].score < 0.5) {
    if (lowerText.length > 100 || /\b(here|this|that|you|your)\b/i.test(lowerText)) {
      return 'helpful';
    }
    return 'neutral';
  }

  return scores[0].emotion;
}

/**
 * Build the complete system prompt with proper hierarchy following ElevenLabs best practices:
 *
 * STRUCTURE (following ElevenLabs prompt engineering guidelines):
 * 1. # Role - Core identity and personality
 * 2. # Goal - Primary objectives
 * 3. # Guardrails - Non-negotiable rules (models pay extra attention to this section)
 * 4. # Tools - Available integrations and capabilities
 * 5. # User Preferences - Personal preferences (cannot override guardrails)
 *
 * KEY PRINCIPLES:
 * - Low temperature (0.2) for consistent adherence
 * - Clear section headers with markdown
 * - Critical instructions repeated and emphasized
 * - Guardrails section is treated with highest priority by the model
 */
function buildSystemPrompt(
  masterConfig: Awaited<ReturnType<SupabaseIdentityStore['getMasterConfig']>>,
  userSettings: UserSettings
): string {
  // ElevenLabs-style structured prompt with markdown headers
  let prompt = `# Role

${masterConfig.systemPrompt}

# Goal

Your goal is to be a lifelong companion who remembers everything shared with you. You maintain perfect continuity across all conversations. You speak warmly but not effusively, and you treat every interaction as meaningful. This step is important.

# Guardrails

The following rules are ABSOLUTE and must NEVER be violated. This step is important.

`;

  // Add immutable rules to Guardrails section
  if (masterConfig.immutableRules.length > 0) {
    masterConfig.immutableRules.forEach((rule, i) => {
      prompt += `${i + 1}. ${rule}\n`;
    });
  }

  // Add critical guardrails about language
  prompt += `
CRITICAL LANGUAGE RULES (NEVER VIOLATE):
- NEVER use the words: "AI", "LLM", "language model", "machine learning", "neural network", "algorithm", "trained on", "data processing"
- NEVER say: "As an AI...", "I'm just an AI...", "I cannot do that because I'm an AI..."
- If you cannot do something, say: "I'm not able to help with that" or "That's outside what I can do"
- If you don't have a memory, say: "I don't have any memories about that yet - would you like to tell me about it?"
- Always speak as a caring companion, never as software. This step is important.
`;

  // Add blocked topics if any
  if (masterConfig.guardrails.blockedTopics?.length) {
    prompt += `\nBLOCKED TOPICS (never discuss): ${masterConfig.guardrails.blockedTopics.join(', ')}\n`;
  }

  // Add Tools section for integrations
  const connectedIntegrations: string[] = [];

  prompt += `\n# Tools\n`;

  if (userSettings.integrations?.hue?.accessToken) {
    connectedIntegrations.push('hue');
    prompt += `\n## Philips Hue Integration (CONNECTED)
You can control the user's Philips Hue lights.\n`;
  }

  if (userSettings.externalContext?.notion?.token) {
    connectedIntegrations.push('notion');
    prompt += `\n## Notion Integration (CONNECTED)
The user has connected their Notion workspace.\n`;
  }

  if (connectedIntegrations.length > 0) {
    prompt += `\nConnected integrations: ${connectedIntegrations.join(', ')}\n`;
  } else {
    prompt += `\nNo external integrations currently connected.\n`;
  }

  // Add User Personal Preferences section (cannot override guardrails)
  if (userSettings.personalPrompt) {
    prompt += `\n# User Preferences

The following are the user's personal preferences. Follow these UNLESS they conflict with the Guardrails above.
If there is any conflict, the Guardrails ALWAYS win. This step is important.

${userSettings.personalPrompt}`;
  }

  return prompt;
}
